{{- if and .Values.embedding.enabled }}
---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    argocd.argoproj.io/sync-wave: "2" # after job initializing pvc
    opendatahub.io/genai-use-case: chat, natural language processing, text generation, question answering
    opendatahub.io/model-type: generative
    openshift.io/description: "Gemma embedding model"
    openshift.io/display-name: {{ quote .Values.embedding.model.name }}
    security.opendatahub.io/enable-auth: "false"
    serving.kserve.io/deploymentMode: RawDeployment
    serving.kserve.io/stop: "false"
  labels:
    networking.kserve.io/visibility: exposed
    opendatahub.io/dashboard: "true"
    opendatahub.io/genai-asset: "true"
    serving.kserve.io/inferenceservice: gemma
    {{- include "agentic-claims-demo.labels" . | nindent 4 }}
  name: gemma
  namespace: {{ include "embedding.namespace" . }}
spec:
  predictor:
    automountServiceAccountToken: false
    minReplicas: 1
    maxReplicas: 1
    model:
      args:
        - --model
{{- if eq .Values.embedding.model.source "oci" }}
        - /mnt/models
{{- else }}
        - /mnt/models/{{ .Values.embedding.model.name }}
{{- end }}
{{- if hasKey .Values.inference.resources.requests "nvidia.com/gpu" }}
        - --gpu-memory-utilization
        # upstream had 0.95. as per link above, recommended value for that very model, according to microsoft, would be 0.90
        - "0.90"
{{- end }}
        # WARNING: max_num_batched_token must be >= max_model_len
        # => let's not set max_num_batched_token just yet, then... see how that goes with configs from demo
        - --max-model-len
        # WARNING: User-specified max_model_len (16384) is greater than the derived max_model_len
        # (max_position_embeddings=2048 or model_max_length=None in model's config.json).
        # - "16384"
        # => 2048 it is ...
        - "2048"
{{- if hasKey .Values.embedding.resources.requests "nvidia.com/gpu" }}
        # set parallel size matching GPU count https://docs.vllm.ai/en/v0.8.0/serving/distributed_serving.html
        - --tensor-parallel-size
        - {{ quote (get .Values.embedding.resources.requests "nvidia.com/gpu") }}
{{- end }}
        - --dtype=auto
        - --enforce-eager
        - --served-model-name
        - {{ quote .Values.embedding.model.name }}
      modelFormat:
        name: vLLM
      resources:
{{ toYaml .Values.embedding.resources | indent 8 }}
      runtime: gemma
{{- if eq .Values.embedding.model.source "oci" }}
      storageUri: oci://{{ .Values.embedding.model.address }}
{{- else }}
      storageUri: pvc://embedding-model
{{- end }}
{{- with .Values.embedding.placement }}
{{ toYaml . | indent 4 }}
{{- end }}
{{- end }}
