{{- if and .Values.inference.enabled }}
---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    argocd.argoproj.io/sync-wave: "2" # after job initializing pvc
    opendatahub.io/genai-use-case: chat, natural language processing, text generation, question answering
    opendatahub.io/model-type: generative
    openshift.io/description: "Llama model with tool calling enabled"
    openshift.io/display-name: {{ quote .Values.inference.model.name }}
    security.opendatahub.io/enable-auth: "false"
    serving.kserve.io/deploymentMode: RawDeployment
    serving.kserve.io/stop: "false"
  labels:
    networking.kserve.io/visibility: exposed
    opendatahub.io/dashboard: "true"
    opendatahub.io/genai-asset: "true"
    serving.kserve.io/inferenceservice: llama
    {{- include "agentic-claims-demo.labels" . | nindent 4 }}
  # original name too long/Route exposing model can't be created
  # name: llama-instruct-32-3b
  name: llama
  namespace: {{ include "inference.namespace" . }}
spec:
  predictor:
    automountServiceAccountToken: false
    minReplicas: 1
    maxReplicas: 1
    model:
      args:
        - --model
{{- if eq .Values.inference.model.source "oci" }}
        - /mnt/models
{{- else }}
        - /mnt/models/{{ .Values.inference.model.name }}
{{- end }}
{{- if hasKey .Values.inference.resources.requests "nvidia.com/gpu" }}
        - --gpu-memory-utilization
        - "0.95"
{{- end }}
        - --max-model-len
        # requires 2x A100 GPUs
        - "20256"
{{- if hasKey .Values.inference.resources.requests "nvidia.com/gpu" }}
        # set parallel size matching GPU count https://docs.vllm.ai/en/v0.8.0/serving/distributed_serving.html
        - --tensor-parallel-size
        - {{ quote (get .Values.inference.resources.requests "nvidia.com/gpu") }}
{{- end }}
        # Quantization (INT8/w8a8 already baked into model weights)
        - --dtype=auto
        # FP8 KV cache quantization to reduce memory by 50% (model weights are INT8, but KV cache is separate)
        - --kv-cache-dtype
        - fp8
        - --served-model-name
        - {{ quote .Values.inference.model.name }}
        # Disable CUDA graphs to reduce memory usage (INT8 model + 70B is memory intensive)
        - --enforce-eager
        # Tool calling configuration - IMPORTANT!
        - --enable-auto-tool-choice
        - --tool-call-parser
        - llama3_json
        - --chat-template=/opt/app-root/template/tool_chat_template_llama3.2_json.jinja
      modelFormat:
        name: vLLM
      resources:
{{ toYaml .Values.inference.resources | indent 8 }}
      runtime: llama
{{- if eq .Values.inference.model.source "oci" }}
      storageUri: oci://{{ .Values.inference.model.address }}
{{- else }}
      storageUri: pvc://inference-model
{{- end }}
{{- with .Values.inference.placement }}
{{ toYaml . | indent 4 }}
{{- end }}
{{- end }}
