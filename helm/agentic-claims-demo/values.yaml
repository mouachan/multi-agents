---
# =============================================================================
# Global Configuration
# =============================================================================
global:
  # OpenShift cluster domain (replace with your cluster domain)
  clusterDomain: "apps.cluster-xxxxx.xxxxx.sandboxYYYY.opentlc.com"  # Replace with your cluster domain

  # Namespace where all resources will be deployed
  namespace: claims-demo

  # Image registry for custom images
  imageRegistry: quay.io/your-org

  # Image pull policy
  imagePullPolicy: IfNotPresent

  proxies:
    HTTP_PROXY: ""
    NO_PROXY: .svc,.local,127.0.0.1

inference:
  enabled: true
  # deploy in same-namespace by default
  namespace: ""
  model:
    pvc:
      # WARNING: that new model weights 72.7GB?!
      # https://huggingface.co/RedHatAI/Llama-3.3-70B-Instruct-quantized.w8a8/tree/main
      size: 100Gi
      storageClass: ""
    # oci storage is faster to get up and running
    # & slower if you need to re-download model image every couple days
    source: oci
    address: registry.redhat.io/rhelai1/modelcar-llama-3-3-70b-instruct-quantized-w8a8:1.5
    # alternatively, go with a PVC:
    # - make sure to set .hfcli.token first provisioning model into PVC
    # - request access to model (?) https://huggingface.co/RedHatAI/Llama-3.3-70B-Instruct-quantized.w8a8
    #   (maybe not necessary? those from Llama org: definitely)
    # source: huggingface
    # address: RedHatAI/Llama-3.3-70B-Instruct-quantized.w8a8
    name: llama-3-3-70b-instruct-quantized-w8a8
  resources:
    limits:
      cpu: "8"
      memory: 16Gi
      nvidia.com/gpu: "4"
    requests:
      cpu: "8"
      memory: 16Gi
      nvidia.com/gpu: "4"
  placement: {}

embedding:
  enabled: true
  # deploy in same-namespace by default
  namespace: ""
  model:
    pvc:
      size: 10Gi
      storageClass: ""
    # https://docs.redhat.com/en/documentation/red_hat_ai_inference_server/3.0/html/validated_models/red_hat_ai_validated_models
    # no OCI version for that model? then again, I'm "guessing" which model to use here ...
    source: huggingface
    # - make sure to set .hfcli.token firs provisioning model into PVC
    # - request access to model: https://huggingface.co/google/embeddinggemma-300m
    address: google/embeddinggemma-300m
    name: embeddinggemma-300m
  resources:
    limits:
      cpu: "2"
      memory: 8Gi
      nvidia.com/gpu: "1"
    requests:
      cpu: "1"
      memory: 8Gi
      nvidia.com/gpu: "1"
  placement: {}

# =============================================================================
# PostgreSQL Database
# =============================================================================
postgresql:
  enabled: true

  # StatefulSet configuration
  # not sure that one makes sense. unless using a PostgresCluster/crunchy or some cluster-capable setup:
  # scaling postgres to 2 replicas isn't as easy as changing this
  replicas: 1

  # Authentication
  auth:
    username: claims_user
    database: claims_db
    # Password will be generated or provided via secret
    existingSecret: postgresql-secret
    secretKeys:
      userPasswordKey: POSTGRES_PASSWORD
      userNameKey: POSTGRES_USER
      databaseKey: POSTGRES_DATABASE

  # Storage
  persistence:
    enabled: true
    size: 10Gi
    storageClass: ""  # Use default storage class

  # Resources
  resources:
    requests:
      cpu: 250m
      memory: 1Gi
    limits:
      cpu: 2000m
      memory: 4Gi

  # Image
  image:
    repository: postgresql
    tag: latest

# =============================================================================
# LlamaStack
# =============================================================================
llamastack:
  enabled: true
  fromRhoai: true  # Use RHOAI LlamaStack service name

  # Route (OpenShift)
  route:
    enabled: true
    host: ""  # Auto-generated if empty
    tls:
      enabled: true  # Enable TLS edge termination

  # Model configuration
  model:
    # endpoint defaults to that of InferenceService deployed as part of this Chart
    # endpoint: "https://llama-3-3-70b-llama-3-3-70b.apps.CLUSTER_DOMAIN/v1"
    apiToken: fake

  # Embedding model
  embedding:
    # endpoint defaults to that of InferenceService deployed as part of this Chart
    # endpoint: "https://embeddinggemma-300m-edg-demo.apps.CLUSTER_DOMAIN/v1"
    dimension: 768

  # Database configuration (LlamaStack uses separate database)
  database:
    name: llamastack
    user: llamastack
    # Password configured via external values file (not in git)

  # Resources
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
    limits:
      cpu: 2000m
      memory: 4Gi

# =============================================================================
# MCP Servers
# =============================================================================
mcp:
  # OCR Server
  ocr:
    enabled: true
    replicas: 1

    image:
      repository: ocr-server
      tag: v1.3

    service:
      type: ClusterIP
      port: 8080

    # Environment variables
    env:
      OCR_LANGUAGES: "en,fr"
      OCR_GPU_ENABLED: "true"
      LOG_LEVEL: "INFO"

    resources:
      requests:
        cpu: 500m
        memory: 2Gi
        nvidia.com/gpu: "1"
      limits:
        cpu: 2000m
        memory: 4Gi
        nvidia.com/gpu: "1"

    placement: {}

  # RAG Server
  rag:
    enabled: true
    replicas: 1

    image:
      repository: rag-server
      tag: v1.3

    service:
      type: ClusterIP
      port: 8080

    # Environment variables
    env:
      LOG_LEVEL: "INFO"

    resources:
      requests:
        cpu: 1000m
        memory: 6Gi
      limits:
        cpu: 2000m
        # had 2Gi: OOMs processing its first claim, llamastack 500s/tells about rag refusing connections as it restarts
        memory: 6Gi

# =============================================================================
# Backend API
# =============================================================================
backend:
  enabled: true

  # Deployment
  # unless PVC has RWX -> can't go beyond 1
  replicas: 1

  # Image
  image:
    repository: backend
    tag: v1.3

  # Service
  service:
    type: ClusterIP
    port: 8000

  # Route (OpenShift)
  route:
    enabled: true
    host: ""  # Auto-generated if empty
    tls:
      enabled: true  # Enable TLS edge termination

  pvc:
    accessMode: ReadWriteOnce
    initPvc: false
    size: 5Gi
    storageClass: ""

  pvc:
    accessMode: ReadWriteOnce
    initPvc: false
    size: 5Gi
    storageClass: ""

  # Environment variables
  env:
    APP_NAME: "Claims Processing Demo"
    ENVIRONMENT: "production"
    DEBUG: "false"
    LOG_LEVEL: "INFO"

  # Admin & Database Reset
  # URL to seed data SQL file (configure to point to your GitHub repository)
  seedDataUrl: "https://raw.githubusercontent.com/mouachan/agentic-claim-demo/refactor/v1.5-architecture/database/seed_data/001_sample_data.sql"

  # Resources
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 1Gi

# =============================================================================
# Frontend
# =============================================================================
frontend:
  enabled: true

  # Deployment
  replicas: 2

  # Image
  image:
    repository: frontend
    tag: v1.3

  # Service
  service:
    type: ClusterIP
    port: 8080

  # Route (OpenShift)
  route:
    enabled: true
    host: ""  # Auto-generated if empty
    tls:
      enabled: true  # Enable TLS edge termination

  # Resources
  resources:
    requests:
      cpu: 200m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi

hfcli:
  image:
    repository: hfcli
    tag: v1.3
  resources:
    requests:
      cpu: "1"
      memory: 4Gi
    limits:
      cpu: "1200m"
      memory: 4Gi
  # only required when pulling models from HF
  # leaving placeholder emptying value skips those jobs
  token: hf_<your-token>

# =============================================================================
# Secrets
# =============================================================================
secrets:
  # PostgreSQL password
  # IMPORTANT: Replace with your own password!
  postgresPassword: "REPLACE_WITH_PASSWORD"
  # loading vector extension requires admin privileges/couldn't run init with regular user
  postgresAdminPassword: "REPLACE_WITH_ADMIN_PASSWORD"

  # Create secrets automatically
  create: true

# =============================================================================
# MinIO (S3-compatible object storage for Data Science Pipelines)
# =============================================================================
minio:
  enabled: false  # Disabled by default - required for RHOAI Data Science Pipelines

  # Image configuration
  image:
    repository: quay.io/minio/minio
    tag: latest

  # Root credentials
  rootUser: "admin"
  rootPassword: "ClaimsDemo2026!"

  # Service configuration
  service:
    type: ClusterIP
    apiPort: 9000
    consolePort: 9001

  # Route configuration
  route:
    api:
      enabled: true
      host: ""  # Auto-generated if empty
      tls:
        enabled: true  # Enable TLS edge termination
    console:
      enabled: true
      host: ""  # Auto-generated if empty
      tls:
        enabled: true  # Enable TLS edge termination

  # Persistence
  persistence:
    enabled: true
    size: 20Gi
    accessMode: ReadWriteOnce
    storageClass: ""  # Use default storage class if empty

  # Resources
  resources:
    requests:
      cpu: 250m
      memory: 512Mi
    limits:
      cpu: 500m
      memory: 1Gi

# =============================================================================
# Data Science Pipelines (RHOAI Kubeflow Pipelines)
# =============================================================================
dataSciencePipelines:
  enabled: false  # Disabled by default - requires RHOAI 3.0+ and MinIO

  # DSP version (v1 or v2)
  dspVersion: v2

  # API Server configuration
  apiServer:
    enableSamplePipeline: false
    enableOauth: true
    resources:
      requests:
        cpu: 200m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 512Mi

  # Database configuration
  database:
    useExternal: true  # Use external PostgreSQL instead of internal MariaDB

    # External PostgreSQL configuration
    externalDB:
      host: postgresql-service
      port: "5432"
      username: pipelines_user
      pipelineDBName: pipelines_db
      # Password will be stored in a secret

    # Internal MariaDB configuration (used if useExternal: false)
    mariaDB:
      pipelineDBName: mlpipeline
      pvcSize: 10Gi
      username: mlpipeline
      resources:
        requests:
          cpu: 200m
          memory: 512Mi
        limits:
          cpu: 500m
          memory: 1Gi

  # Object Storage (MinIO) configuration
  objectStorage:
    useInternalMinio: true  # Use the MinIO deployed by this chart
    enableExternalRoute: false
    basePath: ""
    bucket: pipelines
    host: ""  # Auto-configured if useInternalMinio: true
    port: ""
    region: us-east-1
    scheme: http
    s3CredentialsSecret:
      accessKey: AWS_ACCESS_KEY_ID
      secretKey: AWS_SECRET_ACCESS_KEY
      secretName: ""  # Auto-configured if useInternalMinio: true

  # Persistence Agent configuration
  persistenceAgent:
    numWorkers: 2
    resources:
      requests:
        cpu: 200m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 512Mi

  # Scheduled Workflow configuration
  scheduledWorkflow:
    cronScheduleTimezone: UTC
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 256Mi

  # Pod-to-pod TLS
  podToPodTLS: true

# =============================================================================
# Guardrails (TrustyAI)
# =============================================================================
guardrails:
  enabled: true  # Enable PII detection with TrustyAI guardrails

  name: claims-guardrails
  configMapName: claims-guardrails-config

  # GuardrailsOrchestrator configuration
  enableBuiltInDetectors: true  # Enable HAP, PII detectors as sidecars
  enableGuardrailsGateway: false
  replicas: 1
  logLevel: "info"

  tls:
    enabled: false

  # Chat generation service (LlamaStack)
  chatGeneration:
    hostname: claims-llamastack-service.claims-demo.svc.cluster.local
    port: 8321

  # Detector configurations for ConfigMap
  detectors:
    regex:
      threshold: 0.5
    hap:
      threshold: 0.5
    pii:
      threshold: 0.7

    # Llama Guard 3 1B detector InferenceService
    llamaGuard:
      enabled: true
      minReplicas: 1
      maxReplicas: 1
      storageUri: "oci://quay.io/rh-aiservices-bu/llama-guard-3-1b-modelcar:2.0.0"
      resources:
        requests:
          cpu: "1"
          memory: 8Gi
          nvidia.com/gpu: "1"
        limits:
          cpu: "8"
          memory: 10Gi
          nvidia.com/gpu: "1"
      tolerations:
        - effect: NoSchedule
          operator: Exists
      placement: {}

# =============================================================================
# Tekton Pipelines (RHOAI Data Science Pipelines)
# =============================================================================
tekton:
  enabled: false  # Disabled by default - requires Tekton/OpenShift Pipelines

  # Embedding model for generating vectors
  embeddingModel: gemma-300m

  # Workspace PVC for pipeline tasks
  workspace:
    size: 5Gi
    storageClass: ""  # Use default storage class
