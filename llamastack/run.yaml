version: 2
image_name: rh-dev

apis:
- agents
- datasetio
- files
- inference
- safety
- eval
- scoring
- tool_runtime
- vector_io

providers:
  inference:
  # GPT-OSS 120B via llm-d on OpenShift
  - provider_id: gpt-oss
    provider_type: remote::vllm
    config:
      url: ${env.LLM_URL:=https://openshift-ai-inference.apps.example.com/gpt-oss-120b/gpt-oss-120b/v1}
      max_tokens: 4096
      api_token: ${env.LLM_API_KEY:=fake}
      tls_verify: false
  # Gemma Embedding 300M on OpenShift
  - provider_id: gemma-embedding
    provider_type: remote::vllm
    config:
      url: ${env.EMBEDDING_URL:=https://gemma-embeddinggemma-300m.apps.example.com/v1}
      max_tokens: 512
      api_token: ${env.EMBEDDING_API_KEY:=fake}
      tls_verify: false

  vector_io:
  - provider_id: pgvector
    provider_type: remote::pgvector
    config:
      host: postgresql
      port: 5432
      db: claims_db
      user: claims_user
      password: claims_pass
      persistence:
        backend: kv_default
        namespace: vector_io::pgvector

  safety:
  - provider_id: llama-guard
    provider_type: inline::llama-guard
    config:
      excluded_categories: []

  agents:
  - provider_id: meta-reference
    provider_type: inline::meta-reference
    config:
      persistence:
        agent_state:
          backend: kv_default
          namespace: agents::meta_reference
        responses:
          backend: sql_default
          table_name: agents_responses
          max_write_queue_size: 10000
          num_writers: 4

  eval:
  - provider_id: meta-reference-eval
    provider_type: inline::meta-reference
    config:
      kvstore:
        backend: kv_default
        namespace: eval

  datasetio:
  - provider_id: huggingface
    provider_type: remote::huggingface
    config:
      kvstore:
        backend: kv_default
        namespace: datasetio::huggingface

  scoring:
  - provider_id: basic
    provider_type: inline::basic
    config: {}
  - provider_id: llm-as-judge
    provider_type: inline::llm-as-judge
    config: {}

  tool_runtime:
  - provider_id: rag-runtime
    provider_type: inline::rag-runtime
    config: {}
  - provider_id: model-context-protocol
    provider_type: remote::model-context-protocol
    config: {}

  files:
  # Local dev: localfs. Kubernetes: remote::s3 pointing to MinIO
  # To use S3 locally, add MinIO to docker-compose and use:
  #   provider_type: remote::s3
  #   config:
  #     bucket_name: claims
  #     region: us-east-1
  #     endpoint_url: http://minio:9000
  #     aws_access_key_id: ${env.AWS_ACCESS_KEY_ID}
  #     aws_secret_access_key: ${env.AWS_SECRET_ACCESS_KEY}
  - provider_id: meta-reference-files
    provider_type: inline::localfs
    config:
      storage_dir: /data/files
      metadata_store:
        backend: sql_default
        table_name: files_metadata

  batches:
  - provider_id: reference
    provider_type: inline::reference
    config:
      kvstore:
        namespace: batches
        backend: kv_default

storage:
  backends:
    kv_default:
      type: kv_postgres
      host: ${env.POSTGRES_HOST:=postgresql}
      port: ${env.POSTGRES_PORT:=5432}
      db: ${env.POSTGRES_DB:=claims_db}
      user: ${env.POSTGRES_USER:=claims_user}
      password: ${env.POSTGRES_PASSWORD:=claims_pass}
      table_name: llamastack_kvstore
    sql_default:
      type: sql_postgres
      host: ${env.POSTGRES_HOST:=postgresql}
      port: ${env.POSTGRES_PORT:=5432}
      db: ${env.POSTGRES_DB:=claims_db}
      user: ${env.POSTGRES_USER:=claims_user}
      password: ${env.POSTGRES_PASSWORD:=claims_pass}
  stores:
    metadata:
      backend: kv_default
      namespace: registry
    inference:
      table_name: inference_store
      backend: sql_default
      max_write_queue_size: 10000
      num_writers: 4
    conversations:
      table_name: openai_conversations
      backend: sql_default
    prompts:
      namespace: prompts
      backend: kv_default

registered_resources:
  models:
  # GPT-OSS 120B (MXFP4, TP=4, tool calling)
  - metadata: {}
    model_id: gpt-oss-120b
    provider_id: gpt-oss
    provider_model_id: gpt-oss-120b
    model_type: llm
  # Embeddings - Google Gemma 300M (768-dim)
  - metadata:
      embedding_dimension: 768
    model_id: nomic-embed-text
    provider_id: gemma-embedding
    provider_model_id: embeddinggemma-300m
    model_type: embedding

  shields: []
  vector_dbs: []
  datasets: []
  scoring_fns: []
  benchmarks: []

  tool_groups:
  - toolgroup_id: mcp::ocr-server
    provider_id: model-context-protocol
    mcp_endpoint:
      uri: http://ocr-server:8080/sse
  - toolgroup_id: mcp::rag-server
    provider_id: model-context-protocol
    mcp_endpoint:
      uri: http://rag-server:8080/sse
  - toolgroup_id: mcp::claims-server
    provider_id: model-context-protocol
    mcp_endpoint:
      uri: http://claims-server:8080/sse
  - toolgroup_id: mcp::tenders-server
    provider_id: model-context-protocol
    mcp_endpoint:
      uri: http://tenders-server:8080/sse
  - toolgroup_id: builtin::rag
    provider_id: rag-runtime

telemetry:
  enabled: true

server:
  port: 8321
