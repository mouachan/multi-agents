apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama-3-3-70b
  namespace: llama-3-3-70b
  annotations:
    opendatahub.io/genai-use-case: chat, natural language processing, text generation, question answering, parallel tool calling
    opendatahub.io/hardware-profile-name: nvidia-gpu-l40
    opendatahub.io/hardware-profile-namespace: redhat-ods-applications
    opendatahub.io/model-type: generative
    openshift.io/description: "Llama 3.3 70B INT8 with parallel tool calling support - 4 GPU L4 tensor parallel"
    openshift.io/display-name: llama-3-3-70b
    security.opendatahub.io/enable-auth: "false"
    serving.kserve.io/deploymentMode: RawDeployment
    serving.kserve.io/stop: "false"
  labels:
    networking.kserve.io/visibility: exposed
    opendatahub.io/dashboard: "true"
    opendatahub.io/genai-asset: "true"
spec:
  predictor:
    automountServiceAccountToken: false
    minReplicas: 1
    maxReplicas: 1
    model:
      modelFormat:
        name: vLLM
      name: ""
      runtime: llama-3-3-70b
      storageUri: pvc://pvc-llama-3-3-70b
      args:
        - --model
        - /mnt/models/llama-3-3-70b
        # GPU Configuration for 4x L4 (23GB each = 92GB total for ~70GB INT8 model)
        - --tensor-parallel-size
        - "4"
        - --gpu-memory-utilization
        - "0.9"
        # Quantization (INT8/w8a8 already baked into model weights)
        - --dtype=auto
        # FP8 KV cache quantization to reduce memory by 50% (model weights are INT8, but KV cache is separate)
        - --kv-cache-dtype
        - fp8
        # Context length (Llama 3.3 supports 128k, but we limit for memory)
        # Reduced to 20256 to fit KV cache in available GPU memory
        - --max-model-len
        - "20256"
        # Disable CUDA graphs to reduce memory usage (INT8 model + 70B is memory intensive)
        - --enforce-eager
        # Tool calling configuration (enable parallel tool calls)
        - --enable-auto-tool-choice
        - --tool-call-parser
        - llama3_json
        # Served model name
        - --served-model-name
        - llama-3-3-70b
      resources:
        requests:
          cpu: "8"
          memory: 16Gi
          nvidia.com/gpu: "4"
        limits:
          cpu: "8"
          memory: 16Gi
          nvidia.com/gpu: "4"
    nodeSelector:
      topology.kubernetes.io/zone: eu-central-1a
    tolerations:
      - key: nvidia.com/gpu
        operator: Equal
        value: NVIDIA-L40-PRIVATE
        effect: NoSchedule
